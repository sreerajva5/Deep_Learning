{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a form of machine learning uses a model of computing inspired by the structure of brain.\n",
    "\n",
    "#### Perceptron - artificial neuron\n",
    "Each neuron has a set of inputs, each of which is given a specific weight. The neuron computes some function on these weighted inputs and gives the output.\n",
    "(It learn from the model and reduces error)\n",
    "eg: It can be used to implement Logic Gates.\n",
    "\n",
    "- Training a Perceptron\n",
    "By training we are trying to find a line or plane or hyperplane which can correctly separate two classes by adjusting the weights and biases. We train the perceptron to respond to each input vector with a corresponding target value 0 or 1.\n",
    "\n",
    "- Perceptron Learning Algorithm (steps)\n",
    "1. Initialize the weights and threshold\n",
    "2. Provide the input and calculate the output\n",
    "3. Update the weights\n",
    "(Wj (t) - old weight, Wj (t+1) - updated weight, x- input, d-desired output, y- actual output)\n",
    "Wj (t+1) = Wj (t) + n(d-y)x\n",
    "4. Repeat step 2 & 3 (Error will be calculating and updating the weights again te reduce error).\n",
    "\n",
    "We can estimate the weight values for our training data using 'stochastic gradient descent' optimizer which need below 2 parameters.\n",
    "- Learning rate\n",
    "Used to limit the amount each weight is corrected each time it is updated.\n",
    "- Epochs\n",
    "The number of times to run through the training data while updating the weight.\n",
    "These two parameters will be arguments to the function along with training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "A loss function measures how far apart the current model is from the provided data. Helps to identify how good is the model.\n",
    "\n",
    "MSE (Mean squared error) for regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sreeraj.va\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "linear_model = W*x + b\n",
    "\n",
    "squared_delta = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_delta)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Optimizer\n",
    "\n",
    "Optimizer modifies each variable according to the magnitude of the derivative of loss with respect to that variable.\n",
    "Means the loss is reducing and reducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.54620796], dtype=float32), array([-0.20577921], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([.5], tf.float32)\n",
    "b = tf.Variable([.1], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "linear_model = W*x + b\n",
    "\n",
    "squared_delta = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_delta)   # loass function\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) # learning rate (step size) @ 0.01\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(3):\n",
    "    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "    \n",
    "print(sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W and b changes from 0.5, 0.1 to -0.54, -0.2\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  Its is very important to find optimal learning rate as very low learning rate takes time for the model to learn and very high learning rate cause to miss the minimal error point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
